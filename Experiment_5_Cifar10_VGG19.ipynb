{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment-5-Cifar10-VGG19",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickphatnguyen/CIFAR10-Image-Recognition/blob/master/Experiment_5_Cifar10_VGG19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeS4p_T2THyn",
        "colab_type": "code",
        "outputId": "577692b3-99a0-447d-d2e8-0c11fa72e036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5245
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.initializers import he_normal\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "num_classes  = 10\n",
        "batch_size   = 128\n",
        "epochs       = 200\n",
        "iterations   = 391\n",
        "dropout      = 0.5\n",
        "weight_decay = 0.0001\n",
        "log_filepath = r'./vgg19_retrain_logs/'\n",
        "\n",
        "from keras import backend as K\n",
        "if('tensorflow' == K.backend()):\n",
        "    import tensorflow as tf\n",
        "    from keras.backend.tensorflow_backend import set_session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 80:\n",
        "        return 0.1\n",
        "    if epoch < 160:\n",
        "        return 0.01\n",
        "    return 0.001\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "filepath = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5', WEIGHTS_PATH, cache_subdir='models')\n",
        "\n",
        "# data loading\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# data preprocessing \n",
        "x_train[:,:,:,0] = (x_train[:,:,:,0]-123.680)\n",
        "x_train[:,:,:,1] = (x_train[:,:,:,1]-116.779)\n",
        "x_train[:,:,:,2] = (x_train[:,:,:,2]-103.939)\n",
        "x_test[:,:,:,0] = (x_test[:,:,:,0]-123.680)\n",
        "x_test[:,:,:,1] = (x_test[:,:,:,1]-116.779)\n",
        "x_test[:,:,:,2] = (x_test[:,:,:,2]-103.939)\n",
        "\n",
        "# build model\n",
        "model = Sequential()\n",
        "\n",
        "# Block 1\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv1', input_shape=x_train.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
        "\n",
        "# Block 4\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
        "\n",
        "# Block 5\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
        "\n",
        "# model modification for cifar-10\n",
        "model.add(Flatten(name='flatten'))\n",
        "model.add(Dense(4096, use_bias = True, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc_cifa10'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(4096, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc2'))  \n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))      \n",
        "model.add(Dense(10, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='predictions_cifa10'))        \n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# load pretrained weight from VGG19 by name      \n",
        "model.load_weights(filepath, by_name=True)\n",
        "\n",
        "# -------- optimizer setting -------- #\n",
        "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "checkp = ModelCheckpoint('vgg19.h5',monitor='val_acc',save_best_only=True)\n",
        "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "lrdecay = LearningRateScheduler(scheduler)\n",
        "cbks = [lrdecay,tb_cb,checkp]\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "datagen = ImageDataGenerator(horizontal_flip=True,\n",
        "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                    steps_per_epoch=iterations,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=cbks,\n",
        "                    validation_data=(x_test, y_test))\n",
        "model.save('retrain.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/200\n",
            "391/391 [==============================] - 55s 140ms/step - loss: 2.0661 - acc: 0.6529 - val_loss: 3.1103 - val_acc: 0.4796\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 44s 112ms/step - loss: 1.6763 - acc: 0.7575 - val_loss: 1.8485 - val_acc: 0.7112\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 1.4826 - acc: 0.7920 - val_loss: 1.6917 - val_acc: 0.7149\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 1.3341 - acc: 0.8145 - val_loss: 1.5176 - val_acc: 0.7334\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 1.2220 - acc: 0.8301 - val_loss: 1.2908 - val_acc: 0.8048\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 1.1266 - acc: 0.8419 - val_loss: 1.2884 - val_acc: 0.7843\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 1.0446 - acc: 0.8538 - val_loss: 1.3335 - val_acc: 0.7602\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.9822 - acc: 0.8588 - val_loss: 1.2335 - val_acc: 0.7772\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.9269 - acc: 0.8656 - val_loss: 1.3823 - val_acc: 0.7293\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.8778 - acc: 0.8731 - val_loss: 1.1811 - val_acc: 0.7870\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.8413 - acc: 0.8759 - val_loss: 1.1005 - val_acc: 0.7970\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.8130 - acc: 0.8791 - val_loss: 1.0487 - val_acc: 0.8051\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.7847 - acc: 0.8839 - val_loss: 1.1021 - val_acc: 0.7867\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.7590 - acc: 0.8874 - val_loss: 1.3113 - val_acc: 0.7390\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.7430 - acc: 0.8880 - val_loss: 0.8720 - val_acc: 0.8466\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.7204 - acc: 0.8954 - val_loss: 1.1095 - val_acc: 0.7862\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.7095 - acc: 0.8964 - val_loss: 0.9152 - val_acc: 0.8348\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6919 - acc: 0.8976 - val_loss: 1.1867 - val_acc: 0.7689\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.6853 - acc: 0.8995 - val_loss: 0.8632 - val_acc: 0.8454\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6771 - acc: 0.9024 - val_loss: 0.9109 - val_acc: 0.8352\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6683 - acc: 0.9047 - val_loss: 0.8215 - val_acc: 0.8595\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6660 - acc: 0.9055 - val_loss: 0.9712 - val_acc: 0.8212\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6598 - acc: 0.9069 - val_loss: 0.8227 - val_acc: 0.8553\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6519 - acc: 0.9113 - val_loss: 1.0938 - val_acc: 0.7989\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6553 - acc: 0.9088 - val_loss: 0.9422 - val_acc: 0.8255\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6567 - acc: 0.9094 - val_loss: 0.9097 - val_acc: 0.8393\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6455 - acc: 0.9127 - val_loss: 0.9379 - val_acc: 0.8241\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6489 - acc: 0.9129 - val_loss: 1.1446 - val_acc: 0.7597\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6415 - acc: 0.9162 - val_loss: 0.9127 - val_acc: 0.8364\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6397 - acc: 0.9167 - val_loss: 0.9319 - val_acc: 0.8335\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6472 - acc: 0.9152 - val_loss: 0.8725 - val_acc: 0.8552\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6418 - acc: 0.9187 - val_loss: 0.9401 - val_acc: 0.8329\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6391 - acc: 0.9190 - val_loss: 0.9163 - val_acc: 0.8468\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6419 - acc: 0.9192 - val_loss: 1.0217 - val_acc: 0.8043\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6411 - acc: 0.9217 - val_loss: 0.9952 - val_acc: 0.8147\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6410 - acc: 0.9208 - val_loss: 0.8159 - val_acc: 0.8692\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6390 - acc: 0.9231 - val_loss: 1.1115 - val_acc: 0.7963\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6461 - acc: 0.9200 - val_loss: 0.9305 - val_acc: 0.8399\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6440 - acc: 0.9222 - val_loss: 0.9798 - val_acc: 0.8357\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6438 - acc: 0.9239 - val_loss: 1.0399 - val_acc: 0.8117\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6448 - acc: 0.9244 - val_loss: 0.8965 - val_acc: 0.8590\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6484 - acc: 0.9225 - val_loss: 1.0156 - val_acc: 0.8234\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6486 - acc: 0.9243 - val_loss: 1.2941 - val_acc: 0.7683\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6396 - acc: 0.9272 - val_loss: 0.9589 - val_acc: 0.8385\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6441 - acc: 0.9254 - val_loss: 1.3268 - val_acc: 0.7467\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6455 - acc: 0.9265 - val_loss: 0.9710 - val_acc: 0.8349\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6491 - acc: 0.9245 - val_loss: 0.8947 - val_acc: 0.8631\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6485 - acc: 0.9263 - val_loss: 1.0104 - val_acc: 0.8334\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6565 - acc: 0.9253 - val_loss: 0.8487 - val_acc: 0.8695\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6483 - acc: 0.9273 - val_loss: 0.9345 - val_acc: 0.8473\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6461 - acc: 0.9294 - val_loss: 1.0020 - val_acc: 0.8310\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6493 - acc: 0.9289 - val_loss: 1.0300 - val_acc: 0.8247\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6548 - acc: 0.9256 - val_loss: 0.9505 - val_acc: 0.8420\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.6497 - acc: 0.9296 - val_loss: 1.0679 - val_acc: 0.8159\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6493 - acc: 0.9303 - val_loss: 0.8662 - val_acc: 0.8641\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6566 - acc: 0.9278 - val_loss: 0.9039 - val_acc: 0.8565\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6542 - acc: 0.9295 - val_loss: 0.8463 - val_acc: 0.8733\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6493 - acc: 0.9314 - val_loss: 0.9124 - val_acc: 0.8565\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6550 - acc: 0.9305 - val_loss: 0.9349 - val_acc: 0.8507\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.6503 - acc: 0.9305 - val_loss: 0.9191 - val_acc: 0.8598\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6487 - acc: 0.9314 - val_loss: 0.9270 - val_acc: 0.8567\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6524 - acc: 0.9302 - val_loss: 0.8871 - val_acc: 0.8615\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6502 - acc: 0.9319 - val_loss: 1.0063 - val_acc: 0.8327\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6554 - acc: 0.9302 - val_loss: 1.1877 - val_acc: 0.7900\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6464 - acc: 0.9335 - val_loss: 0.8530 - val_acc: 0.8686\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.6483 - acc: 0.9324 - val_loss: 0.9010 - val_acc: 0.8593\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6543 - acc: 0.9313 - val_loss: 0.9003 - val_acc: 0.8611\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6502 - acc: 0.9335 - val_loss: 0.9674 - val_acc: 0.8410\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6506 - acc: 0.9324 - val_loss: 0.8844 - val_acc: 0.8681\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6662 - acc: 0.9281 - val_loss: 0.9802 - val_acc: 0.8438\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6552 - acc: 0.9339 - val_loss: 0.9425 - val_acc: 0.8575\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6575 - acc: 0.9331 - val_loss: 1.2116 - val_acc: 0.7959\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6585 - acc: 0.9332 - val_loss: 0.9787 - val_acc: 0.8509\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6604 - acc: 0.9336 - val_loss: 1.0367 - val_acc: 0.8201\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.6575 - acc: 0.9344 - val_loss: 0.9337 - val_acc: 0.8561\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6567 - acc: 0.9335 - val_loss: 0.9600 - val_acc: 0.8458\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6604 - acc: 0.9331 - val_loss: 0.8890 - val_acc: 0.8684\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6584 - acc: 0.9348 - val_loss: 0.9770 - val_acc: 0.8509\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 44s 113ms/step - loss: 0.6564 - acc: 0.9349 - val_loss: 0.9920 - val_acc: 0.8499\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 44s 114ms/step - loss: 0.6641 - acc: 0.9338 - val_loss: 1.0856 - val_acc: 0.8098\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.5741 - acc: 0.9622 - val_loss: 0.7306 - val_acc: 0.9159\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.5174 - acc: 0.9788 - val_loss: 0.7173 - val_acc: 0.9198\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.4999 - acc: 0.9821 - val_loss: 0.7072 - val_acc: 0.9222\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 45s 114ms/step - loss: 0.4833 - acc: 0.9855 - val_loss: 0.7081 - val_acc: 0.9225\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 45s 115ms/step - loss: 0.4679 - acc: 0.9884 - val_loss: 0.7050 - val_acc: 0.9244\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.4589 - acc: 0.9892 - val_loss: 0.7093 - val_acc: 0.9227\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.4483 - acc: 0.9908 - val_loss: 0.6950 - val_acc: 0.9249\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.4375 - acc: 0.9924 - val_loss: 0.6945 - val_acc: 0.9251\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.4310 - acc: 0.9923 - val_loss: 0.6884 - val_acc: 0.9256\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.4217 - acc: 0.9931 - val_loss: 0.6855 - val_acc: 0.9261\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.4129 - acc: 0.9944 - val_loss: 0.6900 - val_acc: 0.9254\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.4081 - acc: 0.9936 - val_loss: 0.6777 - val_acc: 0.9262\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.3983 - acc: 0.9950 - val_loss: 0.6845 - val_acc: 0.9252\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3918 - acc: 0.9956 - val_loss: 0.6767 - val_acc: 0.9279\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3855 - acc: 0.9955 - val_loss: 0.6760 - val_acc: 0.9272\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3784 - acc: 0.9962 - val_loss: 0.6705 - val_acc: 0.9277\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3744 - acc: 0.9956 - val_loss: 0.6648 - val_acc: 0.9288\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3675 - acc: 0.9962 - val_loss: 0.6622 - val_acc: 0.9250\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3621 - acc: 0.9960 - val_loss: 0.6625 - val_acc: 0.9257\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 46s 116ms/step - loss: 0.3565 - acc: 0.9964 - val_loss: 0.6625 - val_acc: 0.9265\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3508 - acc: 0.9965 - val_loss: 0.6571 - val_acc: 0.9262\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3448 - acc: 0.9967 - val_loss: 0.6610 - val_acc: 0.9254\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3409 - acc: 0.9964 - val_loss: 0.6502 - val_acc: 0.9265\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3353 - acc: 0.9965 - val_loss: 0.6436 - val_acc: 0.9283\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.3294 - acc: 0.9972 - val_loss: 0.6505 - val_acc: 0.9270\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3252 - acc: 0.9969 - val_loss: 0.6433 - val_acc: 0.9275\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3215 - acc: 0.9964 - val_loss: 0.6332 - val_acc: 0.9282\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3155 - acc: 0.9969 - val_loss: 0.6289 - val_acc: 0.9274\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3106 - acc: 0.9973 - val_loss: 0.6322 - val_acc: 0.9260\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3055 - acc: 0.9974 - val_loss: 0.6387 - val_acc: 0.9264\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.3023 - acc: 0.9973 - val_loss: 0.6282 - val_acc: 0.9277\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2974 - acc: 0.9972 - val_loss: 0.6197 - val_acc: 0.9274\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2922 - acc: 0.9976 - val_loss: 0.6131 - val_acc: 0.9281\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2881 - acc: 0.9975 - val_loss: 0.6258 - val_acc: 0.9262\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2852 - acc: 0.9975 - val_loss: 0.6125 - val_acc: 0.9281\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2803 - acc: 0.9976 - val_loss: 0.6164 - val_acc: 0.9293\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 49s 124ms/step - loss: 0.2773 - acc: 0.9972 - val_loss: 0.6106 - val_acc: 0.9251\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2727 - acc: 0.9975 - val_loss: 0.6010 - val_acc: 0.9250\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2687 - acc: 0.9975 - val_loss: 0.5962 - val_acc: 0.9247\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2648 - acc: 0.9973 - val_loss: 0.6034 - val_acc: 0.9255\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2614 - acc: 0.9975 - val_loss: 0.5876 - val_acc: 0.9271\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2595 - acc: 0.9968 - val_loss: 0.6063 - val_acc: 0.9248\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2547 - acc: 0.9975 - val_loss: 0.6083 - val_acc: 0.9258\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2512 - acc: 0.9975 - val_loss: 0.5841 - val_acc: 0.9286\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2468 - acc: 0.9976 - val_loss: 0.5806 - val_acc: 0.9260\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2442 - acc: 0.9974 - val_loss: 0.5842 - val_acc: 0.9289\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2419 - acc: 0.9971 - val_loss: 0.5886 - val_acc: 0.9233\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2398 - acc: 0.9971 - val_loss: 0.6023 - val_acc: 0.9221\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2339 - acc: 0.9977 - val_loss: 0.5900 - val_acc: 0.9224\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2317 - acc: 0.9974 - val_loss: 0.5766 - val_acc: 0.9281\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2298 - acc: 0.9971 - val_loss: 0.5645 - val_acc: 0.9267\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2267 - acc: 0.9969 - val_loss: 0.5678 - val_acc: 0.9265\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2244 - acc: 0.9968 - val_loss: 0.5540 - val_acc: 0.9270\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2214 - acc: 0.9969 - val_loss: 0.5809 - val_acc: 0.9250\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2182 - acc: 0.9971 - val_loss: 0.5758 - val_acc: 0.9247\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2134 - acc: 0.9978 - val_loss: 0.5617 - val_acc: 0.9259\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2120 - acc: 0.9974 - val_loss: 0.5642 - val_acc: 0.9277\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 46s 119ms/step - loss: 0.2101 - acc: 0.9971 - val_loss: 0.5902 - val_acc: 0.9222\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2083 - acc: 0.9969 - val_loss: 0.5502 - val_acc: 0.9286\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 0.2066 - acc: 0.9963 - val_loss: 0.5450 - val_acc: 0.9247\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 47s 120ms/step - loss: 0.2020 - acc: 0.9970 - val_loss: 0.5586 - val_acc: 0.9235\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.2027 - acc: 0.9964 - val_loss: 0.5456 - val_acc: 0.9228\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.2002 - acc: 0.9960 - val_loss: 0.5368 - val_acc: 0.9269\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.1967 - acc: 0.9967 - val_loss: 0.5400 - val_acc: 0.9234\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.1941 - acc: 0.9966 - val_loss: 0.5398 - val_acc: 0.9237\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 45s 116ms/step - loss: 0.1918 - acc: 0.9968 - val_loss: 0.5517 - val_acc: 0.9232\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 46s 116ms/step - loss: 0.1923 - acc: 0.9961 - val_loss: 0.5479 - val_acc: 0.9211\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 46s 118ms/step - loss: 0.1891 - acc: 0.9963 - val_loss: 0.5370 - val_acc: 0.9229\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.1858 - acc: 0.9965 - val_loss: 0.5369 - val_acc: 0.9234\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.1844 - acc: 0.9965 - val_loss: 0.5104 - val_acc: 0.9255\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 46s 116ms/step - loss: 0.1817 - acc: 0.9965 - val_loss: 0.5246 - val_acc: 0.9240\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 46s 117ms/step - loss: 0.1800 - acc: 0.9966 - val_loss: 0.5466 - val_acc: 0.9204\n",
            "Epoch 153/200\n",
            "200/391 [==============>...............] - ETA: 20s - loss: 0.1757 - acc: 0.9974Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c7unSJUpJc",
        "colab_type": "code",
        "outputId": "701438f8-8216-48fe-dd95-81d4e7ddc936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 558us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.49465283780097963, 0.9291]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2gq0yP3VGL2",
        "colab_type": "code",
        "outputId": "16e78863-272b-44a2-efd7-23aed1fc24a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1287
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "W0524 08:28:42.169225 140102348257152 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0524 08:28:42.182938 140102348257152 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0524 08:28:42.195748 140102348257152 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0524 08:28:42.208183 140102348257152 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train image shape:    (49000, 32, 32, 3)\n",
            "Train label shape:    (49000,)\n",
            "Validate image shape: (1000, 32, 32, 3)\n",
            "Validate label shape: (1000,)\n",
            "Test image shape:     (10000, 32, 32, 3)\n",
            "Test label shape:     (10000,)\n",
            "Train image shape after add bias column:   (49000, 3073)\n",
            "Val image shape after add bias column:     (1000, 3073)\n",
            "Test image shape after add bias column:    (10000, 3073)\n",
            "\n",
            "##############################################################################################\n",
            "Start training Svm classifier\n",
            "Loop 0 loss 156333.10442697618\n",
            "Loop 100 loss 20936.500804172105\n",
            "Loop 200 loss 2804.446578932533\n",
            "Loop 300 loss 378.99519599248873\n",
            "Loop 400 loss 55.45143725836636\n",
            "Loop 500 loss 12.566509722047083\n",
            "Loop 600 loss 6.30507533299658\n",
            "Loop 700 loss 5.941477328360133\n",
            "Loop 800 loss 5.368295685993585\n",
            "Loop 900 loss 5.443503531174825\n",
            "Loop 1000 loss 5.785099797212821\n",
            "Loop 1100 loss 5.805101773638627\n",
            "Loop 1200 loss 6.112740327551405\n",
            "Loop 1300 loss 5.436959911482827\n",
            "Loop 1400 loss 5.680540058172541\n",
            "Training time: 5.967500686645508\n",
            "Training acc:   35.87755102040816%\n",
            "Validating acc: 37.4%\n",
            "Testing acc:    35.370000000000005%\n",
            "\n",
            "Finding best model for Svm classifier\n",
            "Best validation accuracy: 39.6\n",
            "Best Model parameter, lr = 6e-07, reg = 18000\n",
            "Training acc:   37.09387755102041%\n",
            "Validating acc: 39.6%\n",
            "Testing acc:    36.63%\n",
            "\n",
            "##############################################################################################\n",
            "Start training Softmax classifier\n",
            "Loop 0 loss 150972.8667580296\n",
            "Loop 100 loss 20225.948115440668\n",
            "Loop 200 loss 2709.4606645546646\n",
            "Loop 300 loss 364.0537428193538\n",
            "Loop 400 loss 50.52934442622046\n",
            "Loop 500 loss 8.556691826973946\n",
            "Loop 600 loss 3.0380228512797536\n",
            "Loop 700 loss 2.2505191644837397\n",
            "Loop 800 loss 2.162019892864191\n",
            "Loop 900 loss 2.0987898704064403\n",
            "Loop 1000 loss 2.160474386577686\n",
            "Loop 1100 loss 2.1493039136046805\n",
            "Loop 1200 loss 2.100843074500531\n",
            "Loop 1300 loss 2.0911108118229063\n",
            "Loop 1400 loss 2.1366619735991716\n",
            "Training time: 5.934947967529297\n",
            "Training acc:   30.934693877551023%\n",
            "Validating acc: 32.300000000000004%\n",
            "Testing acc:    31.25%\n",
            "\n",
            "Finding best model for Softmax classifier\n",
            "Best validation accuracy: 37.5\n",
            "Best Model parameter, lr = 8e-07, reg = 12000\n",
            "Training acc:   36.04081632653061%\n",
            "Validating acc: 37.5%\n",
            "Testing acc:    36.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkn2W7OlVq4L",
        "colab_type": "code",
        "outputId": "5bfad41b-a000-427d-a1e0-a72a2972962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from keras.layers import Dense\n",
        "type(Dense(784, activation='sigmoid',init='glorot_uniform').trainable = True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-db9deb4081f6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    type(Dense(784, activation='sigmoid',init='glorot_uniform').trainable = True))\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMM5J3Ud9v2t",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}